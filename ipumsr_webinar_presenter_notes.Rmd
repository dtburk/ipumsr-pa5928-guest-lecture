---
title: "ipumsr webinar presenter notes"
output: html_document
---


# NO HEADING (1)




# Who we are (2)




# Overview (3)




# Overview (4)




# What is IPUMS? (5)

ipums has grown substantially over the past 20?? years, and our data is organized within 9 projects


# USA (6)




# CPS (7)




# international (8)




# DHS (9)




# health (10)

*is this microdata project??*


# Time Use (11)




# Higher Ed (12)




# nhgis (13)




# NO HEADING (14)




# Overview (15)




# What is ipumsr? (16)

(Metadata such as value labels, variable labels, and detailed variable 
descriptions.)

Initial API support will be for IPUMS USA, with more projects to follow soon.


# Why use ipumsr? (17)

Regarding "One package": Without ipumsr, you'd need to use a variety of
different approaches from different packages to read in and explore IPUMS
microdata (from projects such as IPUMS USA, CPS, and International), IPUMS
aggregate data (from NHGIS or IHGIS), and IPUMS shapefiles. ipumsr provides one
package with a consistent interface for working with all these different types
of IPUMS data.


# One package to rule them all (18)




# Why use ipumsr? (19)

Regarding "More features": The aforementioned IPUMS API support will be the next
big feature. Another potential new feature is adding tools for properly handling
survey weights. Let us know what would be helpful to you via GitHub or email.


# Why use ipumsr? (20)

There are other ways to read IPUMS data into R, but ipumsr is the fastest way 
to read the data in with attached metadata, such as variable and value labels.


# Poll: Have you used ipumsr? (21)




# Installing ipumsr (22)

Now that we've convinced you of how great ipumsr is, you're probably asking 
"How do I get my hands on it?"


# Installing packages used in this webinar (23)




# Downloading your data extract (24)

Make sure to save the data and DDI files in the same location.


# Downloading your data extract (25)

This helper code checks that you have ipumsr installed, and if you do, it reads
in the DDI codebook and data into separate objects. As an aside, in case anyone
was curious, DDI stands for "Data Documentation Initiative" -- the DDI project
sets standards for documenting datasets, and the codebooks for most IPUMS
projects follow this standard.

We'll see this same code pattern in just a moment when we look at how to read 
in your data.


# Loading packages (26)

These packages are used in some of the examples we will walk through.


# Overview (27)




# Read in the data (28)

Here we see the same code pattern from the "R" helper file above, of reading the
metadata from the codebook into an object named "ddi", and using that object to 
read in the data.


# What's in that `ddi` object? (29)




# What's in that `ddi` object? (30)

You don't need to know this to use ipumsr, but it might help you understand 
what is going on when you read in the DDI codebook and then the data.


# What's in my extract again? (31)




# What's in my extract again? (32)




# Overview (33)




# Available metadata (34)




# Available metadata (35)




# A nicer view of metadata (36)

The function ipums_view() makes a nicely-formatted static html page that allows 
you to browse the metadata associated with your data extract.


# Data (37)




# Wrangling value labels (38)




# `as_factor()` (39)




# `as_factor()` (40)




# `lbl_clean()` (41)




# `lbl_clean()` (42)




# `lbl_na_if()` (43)




# `lbl_na_if()` (44)




# `lbl_na_if()` (45)




# `lbl_collapse()` (46)




# `lbl_collapse()` (47)




# `lbl_relabel()` (48)




# `lbl_relabel()` (49)




# `lbl_relabel()` (50)




# Overview (51)




# Phone availability  (52)




# Phone availability (53)




# Interpretation (54)




# Phone availability by education (55)




# Overview (56)




# Getting geographic data (57)

Geographic boundary data is usually found via a "Geography and GIS" link on the
left sidebar of the home page for a data collection, under the heading
"Supplemental Data".


# Loading shape data (58)

The sp package (short for "spatial") has been around since 2005. It is more
established and some other R spatial packages might still assume you are using
sp data structures. The sf package (short for "simple features") is newer, but 
seems to be on the rise as an alternative sp for some use cases.


# Loading shape data (59)

At the risk of oversimplifying, an sf object is basically a tibble or data.frame
with a special geometry column. That simplification helps me understand the 
process of joining the sf object to data.


# Joining shape data (60)

Before joining to shape data, we need to summarize our person level data at the 
level of the geography we are joining to. Thus, the first block of code computes 
the weighted proportion of persons with access to a phone for each CONSPUMA 
unit in each year. Once our data are summarized so that each row represents a 
value for one CONSPUMA unit in one year, we can join to the sf object we loaded 
above.


# Plotting shape data (61)

For more information on IPUMS geographic data, see the ipumsr vignette on
working with geographic data, or one of several collection-specific recorded
webinars on that topic.


# Overview (62)




# API Timeline (63)

The API is not yet publicly available, but we wanted to offer a preview of the 
functionality that will soon be available in ipumsr.

We plan to support creating extracts via API for all of our data collections, 
but that support will be rolled out one collection at a time, to allow us to 
thoroughly test that the extract API is working as expected for each collection.

IPUMS USA is the first collection that will be supported, and we are currently
doing internal testing of the API for USA. We'll put out a call for beta testers
before the end of this year, but feel free to reach out in the meantime if you
want to be added to that list. We expect to open up the USA extract API to all
IPUMS USA users in the first few months of next year, depending on what sort of
issues arise during beta testing.

Another important note is that there is already a public API for the IPUMS NHGIS
collection, which offers access to tabular data from the US Census Bureau, as
well as corresponding geographic boundaries. ipumsr does not yet include
functions for interacting with the NHGIS API, but there is a guide to
interacting with that API in R, which we'll share a link to at the end of these
slides, and we plan to add that functionality to ipumsr sometime next year.


# What can I do with the API? (64)

So, your next question might be, "what will I be able to do with this API?" 
Here's the high-level overview:

You'll be able to:

Define and submit extract requests.

Check the status of a submitted extract, or have R "wait" for an extract to 
finish by periodically checking the status until it is ready to download.

Download completed data extracts.

Get information on past extracts, including the ability to pull down the 
definition of a previous extract, revise that definition, and resubmit it.

And finally, you can save your extract definition to a JSON file, allowing you 
to share the extract definition with other users for collaboration or 
reproducibility. Saving as JSON makes the definition more easily shareable 
across languages, since R is not the only way to interact with the IPUMS API -- 
you can also call the API using a general purpose tool like curl, and IPUMS is 
developing API client tools for Python in parallel with the R client tools that 
will be part of the ipumsr package.


# What can't I do with the API? (65)

The other important question to answer is what the API can't do.

Most importantly, it can't deliver data "on demand" -- extracts are still 
created through the same extract system used by the website, which means you 
have to wait for your extract to process before you can download it.

In other words, the API does not create a separate system of accessing IPUMS
data, but rather provides a programmatic way to create and download extracts
through the existing extract system.

Secondly, you can't use the API to explore what data are available from IPUMS. 
We plan to add a public metadata API, but the timeline on that is more unknown.

A third limitation is that API users will not initially have access to all the
bells and whistles of the extract system, such as the ability to attach 
characteristics of family members such as spouses or children. We plan to add 
these capabilities once the core functionality is well-tested and stable.


# Pipe-friendly example (66)

Now let's get to some example code! We'll start with a brief example that shows 
a typical API workflow using the "pipe" operator from the magrittr package, 
which is often used in conjunction with tidyverse packages such as dplyr.

We start by defining an extract object using the `define_extract()` function, 
and specifying 

- a data collection -- in this case, "usa" 
- an extract description -- "Occupation by sex and age"
- samples -- here we're asking for the 2017 and 2018 American Community Survey 
- and variables -- sex, age, industry and occupation.

The next code chunk shows how we can go from this extract definition to having 
our extract data available in our R session in one piped expression. For any of 
you who are unfamiliar with the pipe operator, it can be read aloud as the word 
"then". So this piped expression says, take "my_extract", *then* submit extract, 
*then* wait for extract, *then* download extract, *then* read in the data using 
`read_ipums_micro()`.

Since we have to wait for the extract to process, this piped expression will 
take a bit of time to execute, depending on the size of your extract, but if we 
have time we will actually run some code like this during the Q&A so you can 
see it in action.


# Pipe-friendly example (67)

And just for fun, here's what the same piped expression looks like with the new
base R pipe, available as of R 4.1.


# Revise and resubmit (68)

The second argument to `get_recent_extracts_info_list()` is how many recent 
extracts to return, up to a maximum of 10. That function always returns a list, 
so we have to subset the first item if we want the extract object itself.


# Revise and resubmit (69)

The `revise_extract()` function allows you to add and remove variables and 
samples, as well as change the description, data format, or data structure of 
your extract definition.


# Get a tibble of info on recent extracts (70)

If you want to view or operate on a tibble or data.frame representation of your 
recent extracts, there's the `get_recent_extracts_info_tbl()` function.


# Share your extract definition (71)




# Thanks! (72)




