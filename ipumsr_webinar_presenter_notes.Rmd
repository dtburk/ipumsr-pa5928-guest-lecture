---
title: "ipumsr webinar presenter notes"
output: html_document
---


# NO HEADING (1)




# Who we are (2)




# Who we are (3)




# Who we are (4)




# Overview (5)




# Overview (6)




# What is IPUMS? (7)

ipums has grown substantially over the past 20?? years, and our data is organized within 9 projects


# NO HEADING (8)




# NO HEADING (9)




# NO HEADING (10)




# NO HEADING (11)




# NO HEADING (12)




# NO HEADING (13)




# NO HEADING (14)




# NO HEADING (15)




# NO HEADING (16)




# So What is Ipums? (17)




# So What is Ipums? (18)

So ipums really is **data** and a whole lot of it. These 9 different projects interact with different types of data and at different scales but they are united in


# Poll: Which IPUMS data collections have you used? (19)




# Overview (20)




# What is ipumsr? (21)

(Metadata such as value labels, variable labels, and detailed variable 
descriptions.)

Initial API support will be for IPUMS USA, with more projects to follow soon.


# Why use ipumsr? (22)

Regarding "One package": Without ipumsr, you'd need to use a variety of
different approaches from different packages to read in and explore IPUMS
microdata (from projects such as IPUMS USA, CPS, and International), IPUMS
aggregate data (from NHGIS or IHGIS), and IPUMS shapefiles. ipumsr provides one
package with a consistent interface for working with all these different types
of IPUMS data.


# One package to rule them all (23)




# Why use ipumsr? (24)

Regarding "More features": The aforementioned IPUMS API support will be the next
big feature. Another potential new feature is adding tools for properly handling
survey weights. Let us know what would be helpful to you via GitHub or email.


# Why use ipumsr? (25)

There are other ways to read IPUMS data into R, but ipumsr is the fastest way 
to read the data in with attached metadata, such as variable and value labels.


# Poll: Have you used ipumsr? (26)




# Installing ipumsr (27)

Now that we've convinced you of how great ipumsr is, you're probably asking 
"How do I get my hands on it?"


# Installing packages used in this webinar (28)




# Downloading your data extract (29)

Make sure to save the data and DDI files in the same location.


# Downloading your data extract (30)

This helper code checks that you have ipumsr installed, and if you do, it reads
in the DDI codebook and data into separate objects. As an aside, in case anyone
was curious, DDI stands for "Data Documentation Initiative" -- the DDI project
sets standards for documenting datasets, and the codebooks for most IPUMS
projects follow this standard.

We'll see this same code pattern in just a moment when we look at how to read 
in your data.


# Loading packages (31)

These packages are used in some of the examples we will walk through.


# Overview (32)




# Read in the data (33)

Here we see the same code pattern from the "R" helper file above, of reading the
metadata from the codebook into an object named "ddi", and using that object to 
read in the data.

--- 

# Read in the data

- Can also just use `read_ipums_micro()`

```{r}
data <- read_ipums_micro("usa_00013.xml")
```

???
You can also just read the data by passing the file path to the DDI codebook to 
the `read_ipums_micro()` function, but it's often helpful to save the codebook 
to it's own object to preserve the original metadata once you start messing 
around with the data.


# What's in that `ddi` object? (34)

So what's in that ddi object? A bunch of information about your extract, but 
perhaps the most important element for understanding and analyzing the data is 
the "var_info" data.frame.


# What's in that `ddi` object? (35)

As we can see here, var_info is just data.frame where each row has information 
about one variable, including the variable name, label, description, and value 
labels, as well as where the variable is located in the fixed-width file.

You don't need to know this to use ipumsr, but it might help you understand 
what is going on when you read in the DDI codebook and then the data.


# What's in my extract again? (36)

So we've read in our data -- how do we go about exploring it?

We could refer back to the description we wrote when creating the extract, 
maybe that will be informative. Let's see (read extract description). Ooh, 
that's not very helpful. I'm guessing this joke hits close to home for some of 
you long-time IPUMS users.


# What's in my extract again? (37)

[Read text on slides]


# Overview (38)




# Available metadata (39)

So you can see here that ipumsr provides function to extract information from 
the DDI object without the need to slice and dice that "var_info" data.frame I 
showed before. Here we grab the label and description for the variable PHONE.


# Available metadata (40)

Similarly, we can print the value labels by pointing to the DDI. We see here 
that the variable PHONE takes four values: no, yes, not applicable, and 
suppressed.


# An interactive view of metadata (41)

For a more browsable view of the metadata, the ipums_view() function makes a
nicely-formatted static html page that allows you to browse the metadata
associated with your data extract.


# Wrangling value labels (42)

IPUMS value labels don't translate perfectly to R factors. The most important 
difference is that in a factor, values always count up from 1. In IPUMS 
variables, the values themselves often encode meaningful information about a 
nested structure, which we'll see with the education variable below.
  
To preserve both the value and label, `ipumsr` imports labelled variables as
`haven::labelled()` objects, but these aren't always the easiest to deal with, 
because they aren't widely supported by functions from base R and other 
packages.

Luckily ipumsr provides helpers that allow you to use information from both the
value and label.


# Group quarters variable (43)

Here we see another way to print value labels, by pointing directly at the 
variable in the data.frame. This only works, however, if the variable is still 
a `haven::labelled()` vector.

How you want to transform any given variable will always depend on what you 
want to do with it, but in the case of GQ, it doesn't look like the values are 
giving us much information, so perhaps we just want to convert directly to a 
factor.


# `as_factor()` (44)

`as_factor()` (from haven) converts a labelled vector directly to a factor by 
using the value labels as the factor levels.

[Read the table a bit]


# `lbl_clean()` (45)

Now let's get the ipumsr helper functions for labelled vectors.

Sometimes we might want to get rid of values with zero frequency before 
converting to factor. Here, the labels for STATEFIP include all 50 states...


# `lbl_clean()` (46)

...but our extract only includes Minnesota, so we might not need to preserve all 
those labels.


# `lbl_na_if()` (47)

In this case, it makes sense to set values 0 and 8 to missing, given the value 
labels we're seeing here.


# `lbl_na_if()` (48)

`lbl_na_if()` supports a special syntax used by a few of ipumsr's label 
functions that allows you to refer to either the value of the variable (using 
the special dot val variable), the label (using dot lbl), or both. Here we use 
dot val to specify that PHONE2 should be set to missing if the original value is 
zero or eight.


Notice in all these examples that we create a new variable for the transformed 
version, so that we don't overwrite the original. This can be useful so that 
you can engage in some trial and error until you get the variable transformed as 
you want it.


# `lbl_na_if()` (49)

Because lbl_na_if works with both values and labels, we could accomplish the 
same thing by referring to the labels. This time, we specify that PHONE3 should 
be set to missing if the original label was "N/A" or "Suppressed".


# `lbl_collapse()` (50)

[Read bullet point]

We see here that values less than 10 indicate missing or no schooling values; 
values between 10 and 19 all capture levels between nursery school and grade 4; 
and so on.


# `lbl_collapse()` (51)

[Read bullet point]
The label collapse function allows you to collapse values based on a 
hierarchical coding scheme. Here we use the integer division operator to group 
values with the same first digit, and label collapse automatically assigns the 
label of the smallest original value. You can interpret this integer division 
expression as "how many times does 10 go into this value?" For original values 
0, 1, and 2, the answer is that 10 goes into the value zero times, so those 
values all receive the same output value, with the label coming from the 
smallest original value. I think the details of what's going on here take a bit 
of time to unpack, but hopefully this gives you a sense of the usefulness and 
power of this function.


# `lbl_relabel()` (52)




# `lbl_relabel()` (53)




# `lbl_relabel()` (54)




# Overview (55)




# Phone availability  (56)

Microdata often needs to be summarized at a higher level for visualization. In 
this case, if we want to make a graph of phone availability over time, we need 
to first summarize at the year level.

The first block of code computes the weighted proportion of persons with access
to a phone in each year. Once our data are summarized so that each row
represents a value for one year, we can make a graph by year.


# Phone availability (57)




# Interpretation (58)




# Phone availability by education (59)




# Overview (60)




# Getting geographic data (61)

Geographic boundary data is usually found via a "Geography and GIS" link on the
left sidebar of the home page for a data collection, under the heading
"Supplemental Data".


# Loading shape data (62)

The sp package (short for "spatial") has been around since 2005. It is more
established and some other R spatial packages might still assume you are using
sp data structures. The sf package (short for "simple features") is newer, but 
seems to be on the rise as an alternative sp for some use cases.


# Loading shape data (63)

At the risk of oversimplifying, an sf object is basically a tibble or data.frame
with a special geometry column. That simplification helps me understand the 
process of joining the sf object to data.


# Joining shape data (64)

Before joining to shape data, we need to summarize our person level data at the 
level of the geography we are joining to. Thus, the first block of code computes 
the weighted proportion of persons with access to a phone for each CONSPUMA 
unit in each year. Once our data are summarized so that each row represents a 
value for one CONSPUMA unit in one year, we can join to the sf object we loaded 
above.


# Plotting shape data (65)

For more information on IPUMS geographic data, see the ipumsr vignette on
working with geographic data, or one of several collection-specific recorded
webinars on that topic.


# Overview (66)




# API Timeline (67)

The API is not yet publicly available, but we wanted to offer a preview of the 
functionality that will soon be available in ipumsr.

We plan to support creating extracts via API for all of our data collections, 
but that support will be rolled out one collection at a time, to allow us to 
thoroughly test that the extract API is working as expected for each collection.

IPUMS USA is the first collection that will be supported, and we are currently
doing internal testing of the API for USA. We'll put out a call for beta testers
before the end of this year, but feel free to reach out in the meantime if you
want to be added to that list. We expect to open up the USA extract API to all
IPUMS USA users in the first few months of next year, depending on what sort of
issues arise during beta testing.

Another important note is that there is already a public API for the IPUMS NHGIS
collection, which offers access to tabular data from the US Census Bureau, as
well as corresponding geographic boundaries. ipumsr does not yet include
functions for interacting with the NHGIS API, but there is a guide to
interacting with that API in R, which we'll share a link to at the end of these
slides, and we plan to add that functionality to ipumsr sometime next year.


# What can I do with the API? (68)

So, your next question might be, "what will I be able to do with this API?" 
Here's the high-level overview:

You'll be able to:

Define and submit extract requests.

Check the status of a submitted extract, or have R "wait" for an extract to 
finish by periodically checking the status until it is ready to download.

Download completed data extracts.

Get information on past extracts, including the ability to pull down the 
definition of a previous extract, revise that definition, and resubmit it.

And finally, you can save your extract definition to a JSON file, allowing you 
to share the extract definition with other users for collaboration or 
reproducibility. Saving as JSON makes the definition more easily shareable 
across languages, since R is not the only way to interact with the IPUMS API -- 
you can also call the API using a general purpose tool like curl, and IPUMS is 
developing API client tools for Python in parallel with the R client tools that 
will be part of the ipumsr package.


# What can't I do with the API? (69)

The other important question to answer is what the API can't do.

Most importantly, it can't deliver data "on demand" -- extracts are still 
created through the same extract system used by the website, which means you 
have to wait for your extract to process before you can download it.

In other words, the API does not create a separate system of accessing IPUMS
data, but rather provides a programmatic way to create and download extracts
through the existing extract system.

Secondly, you can't use the API to explore what data are available from IPUMS. 
We plan to add a public metadata API, but the timeline on that is more unknown.

A third limitation is that API users will not initially have access to all the
bells and whistles of the extract system, such as the ability to attach 
characteristics of family members such as spouses or children. We plan to add 
these capabilities once the core functionality is well-tested and stable.


# Pipe-friendly example (70)

Now let's get to some example code! We'll start with a brief example that shows 
a typical API workflow using the "pipe" operator from the magrittr package, 
which is often used in conjunction with tidyverse packages such as dplyr.

We start by defining an extract object using the `define_extract()` function, 
and specifying 

- a data collection -- in this case, "usa" 
- an extract description -- "Occupation by sex and age"
- samples -- here we're asking for the 2017 and 2018 American Community Survey 
- and variables -- sex, age, industry and occupation.

The next code chunk shows how we can go from this extract definition to having 
our extract data available in our R session in one piped expression. For any of 
you who are unfamiliar with the pipe operator, it can be read aloud as the word 
"then". So this piped expression says, take "my_extract", *then* submit extract, 
*then* wait for extract, *then* download extract, *then* read in the data using 
`read_ipums_micro()`.

Since we have to wait for the extract to process, this piped expression will 
take a bit of time to execute, depending on the size of your extract, but if we 
have time we will actually run some code like this during the Q&A so you can 
see it in action.


# Pipe-friendly example (71)

And just for fun, here's what the same piped expression looks like with the new
base R pipe, available as of R 4.1.


# Revise and resubmit (72)

Another handy workflow where using the extract API is a "revise and resubmit" 
workflow. Often, you might realize that you should have added one more or a few 
more variables to your previous extract, and you just want to resubmit that 
extract with a few additional variables.

To do this with ipumsr functions, you first pull down the definition of your 
most recent extract using the function `get_recent_extracts_info_list()`, which 
can return information on up to 10 recent extracts. Here, we specify that we 
only want one recent extract (the most recent one), but because this function 
always returns a list, we also have to subset the first element.

Alternatively, if we know the extract number of the extract we want to revise, 
we can use `get_extract_info()` and specify a shorthand notation for USA 
extract number 33, for example.


# Revise and resubmit (73)

Once we have the old extract definition, we can pass it to the 
`revise_extract()` function to add a variable, then resubmit it.

The `revise_extract()` function currently allows you to add and remove variables
and samples, as well as change the description, data format, or data structure
of your extract definition.


# Share your extract definition (74)

One thing that really excites us about the API is the ability to share extract
definitions to facilitate collaboration or reproducibility.

To write your extract out to a JSON file, you can use the 
`save_extract_as_json()` function as shown here.

Saving as JSON makes it easier to share an extract definition with another user 
who might not use R -- they can use the JSON definition to submit the extract 
using curl or Python, possibly using the currently in-development IPUMS API 
client tools in the "ipumspy" module.

If you are using ipumsr, the `define_extract_from_json()` function will create
an extract object from a JSON-formatted definition shared by someone else.


# Resources (75)




